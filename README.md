## Polarpipe

Polars-powered ETL playground for large semi-structured files. It generates fake user data, applies lightweight cleaning, and exercises validation/writer flows while keeping memory use low.

Polars Optimization: [PR #4 – Polars Optimization](https://github.com/gnm3000/error-handling-data-project/pull/3)


Key pieces:
- `generation-data/small/` holds checked-in samples (10k rows) generated by `generation-data/generate.py` (JSON/CSV/Parquet).
- `generation-data/large/` is ignored from git; generate locally with `generation-data/generate_large.sh` (NDJSON + Parquet by default).
- `generation-data/generate_large.py` streams 10M+ fake rows to NDJSON without blowing memory and can stream-convert to Parquet/CSV.
- `pipeline.py` lazily scans input (NDJSON/CSV/Parquet/JSON), validates required fields, cleans names/ids, and logs a sample.
- `ingestion/reader.py` and `validator.py` handle input reading and schema checks; `transformer.py` normalizes the data.
- `tests/` holds quick smoke and schema tests to keep things honest.
- `tests/test_benchmark.py` benchmarks Polars vs pandas group-bys with `pytest-benchmark`.
- `scripts/run_quality.sh` runs formatting, lint, types, security, and pytest in one go.
- `evaluate_performance.sh` offers CPU/memory profiling (cProfile, scalene, snakeviz).

## Local Setup

1. Install deps with uv (creates `.venv` automatically): `uv sync --extra dev`
2. Run quality pipeline (uses the venv uv created):
   - Auto-fix and checks: `./scripts/run_quality.sh`
   - Check-only (no edits): `./scripts/run_quality.sh check`

## Publish and install (PyPI + GitHub Packages)

- Publish: create a Release or run the `Publish (GitHub Packages)` workflow manually; it uploads to PyPI via `.github/workflows/publish.yml` using `TWINE_USERNAME=__token__` and `PYPI_TOKEN` in secrets.
- GitHub Packages visibility: GitHub automatically surfaces packages published to PyPI whose metadata points back to this repo.
- Package name in PyPI is currently `data-engineer-application`; update to `polarpipe` when you cut the next release.
- Install from PyPI:
  ```bash
  pip install data-engineer-application
  ```
  or with uv:
  ```bash
  uv pip install data-engineer-application
  ```
  Remember to bump `version` in `pyproject.toml` before publishing a release.


make run

<img width="993" height="398" alt="image" src="https://github.com/user-attachments/assets/818bf893-15f1-45d7-8ce0-2f082579306b" />


This image shows a large NDJSON file is scanned lazily, its schema validated, and a sampled pre-clean analysis is performed (100k rows). Cleanup steps run with minimal memory use. Profiling shows low overhead compared to full materialization. Finally, a small sample of three cleaned records is collected and logged, demonstrating the pipeline’s efficient lazy execution on multi-million-row data.
