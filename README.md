## Data Engineer Application

Small ingestion playground that generates fake user data, applies simple transforms, and exercises a validation/writer flow. It ships with a full quality toolchain to keep code style, typing, and security in check.

Key pieces:
- `generation-data/small/` holds checked-in samples (10k rows) generated by `generation-data/generate.py` (JSON/CSV/Parquet).
- `generation-data/large/` is ignored from git; generate locally with `generation-data/generate_large.sh` (NDJSON + Parquet by default).
- `generation-data/generate_large.py` streams 10M+ fake rows to NDJSON without blowing memory and can stream-convert to Parquet/CSV.
- `pipeline.py` lazily scans input (NDJSON/CSV/Parquet/JSON), validates required fields, cleans names/ids, and logs a sample.
- `ingestion/reader.py` and `validator.py` handle input reading and schema checks; `transformer.py` normalizes the data.
- `tests/` holds quick smoke and schema tests to keep things honest.
- `tests/test_benchmark.py` benchmarks Polars vs pandas group-bys with `pytest-benchmark`.
- `scripts/run_quality.sh` runs formatting, lint, types, security, and pytest in one go.
- `evaluate_performance.sh` offers CPU/memory profiling (cProfile, scalene, snakeviz).

## Local Setup

1. Install deps with uv (creates `.venv` automatically): `uv sync --extra dev`
2. Run quality pipeline (uses the venv uv created):
   - Auto-fix and checks: `./scripts/run_quality.sh`
   - Check-only (no edits): `./scripts/run_quality.sh check`


make run

<img width="993" height="398" alt="image" src="https://github.com/user-attachments/assets/818bf893-15f1-45d7-8ce0-2f082579306b" />


This image shows a large NDJSON file is scanned lazily, its schema validated, and a sampled pre-clean analysis is performed (100k rows). Cleanup steps run with minimal memory use. Profiling shows low overhead compared to full materialization. Finally, a small sample of three cleaned records is collected and logged, demonstrating the pipelineâ€™s efficient lazy execution on multi-million-row data.
